{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f70091",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ğŸ¤– **TRANSFORMERS: The Complete Guide**\n",
    "## From Zero to Hero - Understanding Modern AI\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ“š **TABLE OF CONTENTS**\n",
    "\n",
    "1. **The Problem: Why Do We Need Transformers?**\n",
    "2. **The Big Picture: What is a Transformer?**\n",
    "3. **Self-Attention: The Magic Ingredient**\n",
    "4. **Multi-Head Attention: Multiple Perspectives**\n",
    "5. **Positional Encoding: Adding Order**\n",
    "6. **The Encoder: Understanding Input**\n",
    "7. **The Decoder: Generating Output**\n",
    "8. **Complete Architecture Walkthrough**\n",
    "9. **Real-World Applications**\n",
    "10. **Building a Simple Transformer from Scratch**\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ¯ **PART 1: The Problem - Why Transformers?**\n",
    "\n",
    "## **The Evolution of Sequence Models**\n",
    "\n",
    "### **The Journey:**\n",
    "\n",
    "```\n",
    "1990s: Simple RNN\n",
    "       â†“\n",
    "       Problem: Forgets long sequences\n",
    "       \n",
    "2000s: LSTM & GRU\n",
    "       â†“\n",
    "       Problem: Still struggles with very long texts\n",
    "                Sequential processing (slow!)\n",
    "                \n",
    "2017: TRANSFORMERS ğŸš€\n",
    "       â†“\n",
    "       Solution: Process ALL words at once!\n",
    "                 Remember everything!\n",
    "                 Parallel processing (fast!)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **The RNN/LSTM Problem Visualized**\n",
    "\n",
    "### **Example: Translating a long sentence**\n",
    "\n",
    "```\n",
    "English: \"The cat that was sitting on the mat which was placed \n",
    "          in the living room near the window was sleeping peacefully.\"\n",
    "\n",
    "RNN/LSTM Processing:\n",
    "â”œâ”€ Word 1: \"The\" â†’ Memory: 100% ğŸ’ª\n",
    "â”œâ”€ Word 5: \"sitting\" â†’ Memory: 70% ğŸ“‰\n",
    "â”œâ”€ Word 10: \"mat\" â†’ Memory: 40% ğŸ“‰ğŸ“‰\n",
    "â”œâ”€ Word 15: \"living\" â†’ Memory: 20% ğŸ“‰ğŸ“‰ğŸ“‰\n",
    "â””â”€ Word 20: \"sleeping\" â†’ Memory: 5% ğŸ˜µ\n",
    "                         What was sleeping?? \n",
    "                         (Forgot \"cat\"!)\n",
    "```\n",
    "\n",
    "### **Sequential Processing Problem:**\n",
    "\n",
    "```\n",
    "RNN/LSTM (Sequential - SLOW):\n",
    "Time 1: Process word 1 â†’ Wait\n",
    "Time 2: Process word 2 â†’ Wait\n",
    "Time 3: Process word 3 â†’ Wait\n",
    "...\n",
    "Time 100: Process word 100 â†’ Wait\n",
    "\n",
    "Total time: 100 steps ğŸŒ\n",
    "\n",
    "Transformer (Parallel - FAST):\n",
    "Time 1: Process ALL 100 words simultaneously!\n",
    "\n",
    "Total time: 1 step âš¡\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **The Key Innovation:**\n",
    "\n",
    "> **\"What if we could look at ALL words at the same time and understand which words are related to each other?\"**\n",
    "\n",
    "This is **ATTENTION** - the revolutionary idea! ğŸ¯\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ§  **PART 2: The Big Picture - What is a Transformer?**\n",
    "\n",
    "## **Simple Analogy: The Research Team**\n",
    "\n",
    "Think of a Transformer as a **team of researchers** analyzing a document:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚        ğŸ“š RESEARCH TEAM ANALYZING TEXT          â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                 â”‚\n",
    "â”‚  Input: \"The cat sat on the mat\"               â”‚\n",
    "â”‚                                                 â”‚\n",
    "â”‚  ğŸ‘¥ ENCODER TEAM (Understanding):               â”‚\n",
    "â”‚  â”œâ”€ Researcher 1: \"What's the subject?\"        â”‚\n",
    "â”‚  â”‚   â†’ Looks at ALL words, finds: \"cat\"        â”‚\n",
    "â”‚  â”‚                                              â”‚\n",
    "â”‚  â”œâ”€ Researcher 2: \"What's the action?\"         â”‚\n",
    "â”‚  â”‚   â†’ Looks at ALL words, finds: \"sat\"        â”‚\n",
    "â”‚  â”‚                                              â”‚\n",
    "â”‚  â””â”€ Researcher 3: \"Where?\"                     â”‚\n",
    "â”‚      â†’ Looks at ALL words, finds: \"mat\"        â”‚\n",
    "â”‚                                                 â”‚\n",
    "â”‚  ğŸ‘¥ DECODER TEAM (Generating):                  â”‚\n",
    "â”‚  â”œâ”€ Writer 1: Uses encoder insights            â”‚\n",
    "â”‚  â”œâ”€ Writer 2: Generates translation/response   â”‚\n",
    "â”‚  â””â”€ Writer 3: Checks for coherence             â”‚\n",
    "â”‚            The cat sat on the ? mat   \n",
    "                                                  â”‚\n",
    "â”‚  Output: Translation/Generated Text             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **High-Level Architecture:**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚           INPUT TEXT                    â”‚\n",
    "â”‚    \"The cat sat on the mat\"             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                 â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚    POSITIONAL ENCODING                 â”‚\n",
    "â”‚    (Add word order information)        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                 â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚         ENCODER STACK                  â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚\n",
    "â”‚  â”‚  Self-Attention          â”‚ Ã—N      â”‚\n",
    "â”‚  â”‚  (Words look at words)   â”‚ layers  â”‚\n",
    "â”‚  â”‚         â†“                â”‚         â”‚\n",
    "â”‚  â”‚  Feed Forward Network    â”‚         â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                 â†“\n",
    "         [Understanding of input]\n",
    "                 â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚         DECODER STACK                  â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚\n",
    "â”‚  â”‚  Masked Self-Attention   â”‚ Ã—N      â”‚\n",
    "â”‚  â”‚  (Look at previous words)â”‚ layers  â”‚\n",
    "â”‚  â”‚         â†“                â”‚         â”‚\n",
    "â”‚  â”‚  Encoder-Decoder Attn    â”‚         â”‚\n",
    "â”‚  â”‚  (Look at input)         â”‚         â”‚\n",
    "â”‚  â”‚         â†“                â”‚         â”‚\n",
    "â”‚  â”‚  Feed Forward Network    â”‚         â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                 â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚        OUTPUT PREDICTION               â”‚\n",
    "â”‚    \"Le chat s'est assis sur le tapis\"  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# âœ¨ **PART 3: Self-Attention - The Magic Ingredient**\n",
    "\n",
    "## **What is Attention?**\n",
    "\n",
    "**Simple analogy:** Reading a sentence and highlighting important words\n",
    "\n",
    "```\n",
    "Sentence: \"The cat sat on the mat\"\n",
    "\n",
    "Question: \"What sat?\"\n",
    "\n",
    "Your attention automatically focuses:\n",
    "\"The CAT sat on the mat\"\n",
    "    ^^^\n",
    "   (highlighted in your mind)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Self-Attention Mechanism**\n",
    "\n",
    "### **The Core Idea:**\n",
    "\n",
    "> **Each word looks at EVERY other word and decides how much attention to pay to each one.**\n",
    "\n",
    "---\n",
    "\n",
    "### **Step-by-Step Example:**\n",
    "\n",
    "**Sentence:** \"The cat sat on the mat\"\n",
    "\n",
    "**Processing the word \"sat\":**\n",
    "\n",
    "```\n",
    "\"sat\" asks every word: \"How relevant are you to me?\"\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Word \"sat\" analyzing:                    â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ \"The\"  â†’ Relevance: 0.1 (10%) ğŸ“Š        â”‚\n",
    "â”‚ \"cat\"  â†’ Relevance: 0.9 (90%) ğŸ“ŠğŸ“ŠğŸ“Š    â”‚ â† Subject!\n",
    "â”‚ \"sat\"  â†’ Relevance: 0.3 (30%) ğŸ“Š        â”‚\n",
    "â”‚ \"on\"   â†’ Relevance: 0.2 (20%) ğŸ“Š        â”‚\n",
    "â”‚ \"the\"  â†’ Relevance: 0.1 (10%) ğŸ“Š        â”‚\n",
    "â”‚ \"mat\"  â†’ Relevance: 0.7 (70%) ğŸ“ŠğŸ“Š      â”‚ â† Location!\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Result: \"sat\" pays most attention to \"cat\" and \"mat\"\n",
    "        Understanding: \"cat sat ... mat\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **The Three Components: Query, Key, Value (QKV)**\n",
    "\n",
    "### **Restaurant Analogy:**\n",
    "\n",
    "```\n",
    "Imagine a restaurant:\n",
    "\n",
    "QUERY (Q) = Your order\n",
    "  \"I want pasta\" \n",
    "  \n",
    "KEY (K) = Menu items\n",
    "  \"Pizza, Pasta, Salad, Burger\"\n",
    "  \n",
    "VALUE (V) = Actual dishes\n",
    "  [ğŸ•, ğŸ, ğŸ¥—, ğŸ”]\n",
    "\n",
    "Process:\n",
    "1. Compare your QUERY with all KEYS\n",
    "   \"pasta\" matches \"Pasta\" (high score!)\n",
    "   \"pasta\" doesn't match \"Pizza\" (low score)\n",
    "   \n",
    "2. Get the VALUE based on match\n",
    "   High match with \"Pasta\" â†’ You get ğŸ\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **In Transformers:**\n",
    "\n",
    "```\n",
    "For word \"sat\":\n",
    "\n",
    "QUERY (Q) = \"What am I looking for?\"\n",
    "            \"I'm an action, looking for subject and object\"\n",
    "\n",
    "KEY (K) = \"What does each word represent?\"\n",
    "          \"The\"=article, \"cat\"=noun, \"on\"=preposition, \"mat\"=noun\n",
    "\n",
    "VALUE (V) = \"What information does each word carry?\"\n",
    "            Actual word embeddings/meanings\n",
    "\n",
    "Process:\n",
    "1. Compare Q(\"sat\") with K(all words)\n",
    "   Q(\"sat\") Â· K(\"cat\") = 0.9 (high!)\n",
    "   Q(\"sat\") Â· K(\"the\") = 0.1 (low)\n",
    "   \n",
    "2. Get weighted sum of VALUES\n",
    "   0.9 Ã— V(\"cat\") + 0.1 Ã— V(\"the\") + ...\n",
    "   = Understanding of \"sat\" in context\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Mathematical Formula:**\n",
    "\n",
    "```\n",
    "Attention(Q, K, V) = softmax(QÂ·K^T / âˆšd_k) Â· V\n",
    "\n",
    "Breaking it down:\n",
    "\n",
    "1. QÂ·K^T\n",
    "   â†’ Calculate similarity between query and all keys\n",
    "   â†’ Dot product: How similar are they?\n",
    "\n",
    "2. / âˆšd_k\n",
    "   â†’ Scale by square root of dimension\n",
    "   â†’ Prevents very large numbers\n",
    "\n",
    "3. softmax(...)\n",
    "   â†’ Convert scores to probabilities (0 to 1, sum to 1)\n",
    "   â†’ Higher score = more attention\n",
    "\n",
    "4. Â· V\n",
    "   â†’ Multiply probabilities by values\n",
    "   â†’ Get weighted combination\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Visual Example:**\n",
    "\n",
    "**Processing \"sat\" in \"The cat sat on the mat\":**\n",
    "\n",
    "```\n",
    "Step 1: Calculate Attention Scores\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "       The   cat   sat   on    the   mat\n",
    "sat  [ 0.1,  0.9,  0.3,  0.2,  0.1,  0.7 ]\n",
    "       â†“     â†“     â†“     â†“     â†“     â†“\n",
    "     10%   90%   30%   20%   10%   70%\n",
    "\n",
    "Step 2: Apply Softmax (normalize to sum = 1.0)\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "       The   cat   sat   on    the   mat\n",
    "sat  [ 0.04, 0.41, 0.13, 0.08, 0.04, 0.30 ]\n",
    "\n",
    "Step 3: Weighted Sum of Values\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "New representation of \"sat\":\n",
    "= 0.04 Ã— V(\"The\") + 0.41 Ã— V(\"cat\") + 0.13 Ã— V(\"sat\") +\n",
    "  0.08 Ã— V(\"on\")  + 0.04 Ã— V(\"the\") + 0.30 Ã— V(\"mat\")\n",
    "\n",
    "= Mostly \"cat\" (41%) + \"mat\" (30%) + some \"sat\" (13%)\n",
    "= Understanding: action \"sat\" done by \"cat\" on \"mat\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Complete Attention Matrix:**\n",
    "\n",
    "**All words attending to all words:**\n",
    "\n",
    "```\n",
    "         Attending to â†’\n",
    "         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "         â”‚ The  cat  sat  on   the  mat    â”‚\n",
    "    â”Œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "Fromâ”‚The â”‚ 0.6  0.2  0.1  0.05 0.04 0.01   â”‚\n",
    "    â”‚cat â”‚ 0.1  0.4  0.3  0.05 0.05 0.1    â”‚\n",
    "    â”‚sat â”‚ 0.04 0.41 0.13 0.08 0.04 0.30   â”‚\n",
    "    â”‚on  â”‚ 0.05 0.1  0.2  0.3  0.15 0.2    â”‚\n",
    "    â”‚the â”‚ 0.1  0.05 0.05 0.1  0.5  0.2    â”‚\n",
    "    â”‚mat â”‚ 0.05 0.15 0.1  0.15 0.05 0.5    â”‚\n",
    "    â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Reading the matrix:\n",
    "- Row \"sat\", Column \"cat\" = 0.41\n",
    "  â†’ \"sat\" pays 41% attention to \"cat\"\n",
    "  \n",
    "- Row \"cat\", Column \"mat\" = 0.1\n",
    "  â†’ \"cat\" pays 10% attention to \"mat\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ¨ **PART 4: Multi-Head Attention**\n",
    "\n",
    "## **Why Multiple Heads?**\n",
    "\n",
    "**Single attention head** = One perspective\n",
    "\n",
    "**Multiple attention heads** = Multiple perspectives, like multiple experts!\n",
    "\n",
    "---\n",
    "\n",
    "## **Analogy: Art Critics**\n",
    "\n",
    "```\n",
    "Analyzing a painting:\n",
    "\n",
    "ğŸ‘¨â€ğŸ¨ Critic 1 (Color Head):\n",
    "   Focuses on: colors, shading, tones\n",
    "   \n",
    "ğŸ‘©â€ğŸ¨ Critic 2 (Composition Head):\n",
    "   Focuses on: layout, balance, perspective\n",
    "   \n",
    "ğŸ‘¨â€ğŸ¨ Critic 3 (Emotion Head):\n",
    "   Focuses on: mood, feeling, message\n",
    "   \n",
    "Combined: Complete understanding!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **In Transformers:**\n",
    "\n",
    "**Example: \"The cat sat on the mat\"**\n",
    "\n",
    "```\n",
    "HEAD 1 (Syntax Head):\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Focuses on: Grammar structure   â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ \"The\" â†’ modifies â†’ \"cat\"        â”‚\n",
    "â”‚ \"cat\" â†’ performs â†’ \"sat\"        â”‚\n",
    "â”‚ \"sat\" â†’ location â†’ \"mat\"        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "HEAD 2 (Semantic Head):\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Focuses on: Meaning relations   â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ \"cat\" â†’ animal entity           â”‚\n",
    "â”‚ \"sat\" â†’ action/state            â”‚\n",
    "â”‚ \"mat\" â†’ object/surface          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "HEAD 3 (Position Head):\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Focuses on: Spatial relations   â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ \"on\" â†’ preposition              â”‚\n",
    "â”‚ \"cat\" â†’ subject location        â”‚\n",
    "â”‚ \"mat\" â†’ reference location      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "COMBINED:\n",
    "Complete understanding with multiple perspectives!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Architecture:**\n",
    "\n",
    "```\n",
    "Input Embedding: [512 dimensions]\n",
    "         â†“\n",
    "Split into 8 heads (512 Ã· 8 = 64 dimensions each)\n",
    "         â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Head 1  Head 2  Head 3  ...  Head 8   â”‚\n",
    "â”‚  [64]    [64]    [64]  ...   [64]     â”‚\n",
    "â”‚   â†“       â†“       â†“           â†“       â”‚\n",
    "â”‚  Attn   Attn    Attn   ...   Attn     â”‚\n",
    "â”‚   â†“       â†“       â†“           â†“       â”‚\n",
    "â”‚  [64]    [64]    [64]  ...   [64]     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â†“\n",
    "Concatenate: [64+64+64+...+64 = 512]\n",
    "         â†“\n",
    "Linear Layer: [512]\n",
    "         â†“\n",
    "Output: [512 dimensions]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Why 8 heads in practice?**\n",
    "\n",
    "```\n",
    "Each head learns different patterns:\n",
    "\n",
    "Head 1: Subject-Verb relationships\n",
    "Head 2: Verb-Object relationships  \n",
    "Head 3: Adjective-Noun relationships\n",
    "Head 4: Long-range dependencies\n",
    "Head 5: Positional patterns\n",
    "Head 6: Semantic similarities\n",
    "Head 7: Syntactic structures\n",
    "Head 8: Contextual nuances\n",
    "\n",
    "8 heads = 8 different experts! ğŸ¯\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ“ **PART 5: Positional Encoding**\n",
    "\n",
    "## **The Problem:**\n",
    "\n",
    "Transformers process all words **simultaneously** - no inherent order!\n",
    "\n",
    "```\n",
    "Input: \"Cat eats fish\"\n",
    "Input: \"Fish eats cat\"\n",
    "\n",
    "Without position info: Both look the same! âŒ\n",
    "With position info: Different meanings! âœ…\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Solution: Add Position Information**\n",
    "\n",
    "### **Simple Example:**\n",
    "\n",
    "```\n",
    "Words:     \"The\"  \"cat\"  \"sat\"\n",
    "Positions:   1      2      3\n",
    "\n",
    "Add position to word:\n",
    "\"The\" + position_1 = \"The_at_position_1\"\n",
    "\"cat\" + position_2 = \"cat_at_position_2\"\n",
    "\"sat\" + position_3 = \"sat_at_position_3\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **How It Works:**\n",
    "\n",
    "### **Positional Encoding Formula:**\n",
    "\n",
    "```\n",
    "For each position pos and dimension i:\n",
    "\n",
    "PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))\n",
    "PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "\n",
    "Where:\n",
    "- pos = position in sequence (0, 1, 2, ...)\n",
    "- i = dimension index (0, 1, 2, ..., d_model/2)\n",
    "- d_model = embedding dimension (usually 512)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Why Sin/Cos?**\n",
    "\n",
    "**Properties:**\n",
    "1. **Unique**: Each position gets unique pattern\n",
    "2. **Smooth**: Similar positions have similar encodings\n",
    "3. **Relative**: Can learn relative positions\n",
    "\n",
    "```\n",
    "Position 0:   [0.00, 1.00, 0.00, 1.00, ...]\n",
    "Position 1:   [0.84, 0.54, 0.01, 1.00, ...]\n",
    "Position 2:   [0.91, -0.42, 0.02, 1.00, ...]\n",
    "...\n",
    "\n",
    "Pattern: Waves of different frequencies\n",
    "         Low dimensions: high frequency (change fast)\n",
    "         High dimensions: low frequency (change slow)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Visual Representation:**\n",
    "\n",
    "```\n",
    "Position Encoding Visualization:\n",
    "\n",
    "Dim 0-1 (high freq):  ~~~~~~~~~~~~~~~  Fast wave\n",
    "Dim 2-3 (med freq):   ~~~~~~~~        Medium wave  \n",
    "Dim 4-5 (low freq):   ~~~             Slow wave\n",
    "\n",
    "Combined: Unique signature for each position!\n",
    "\n",
    "Position 0: â–“â–‘â–“â–‘â–“â–‘\n",
    "Position 1: â–‘â–“â–‘â–“â–‘â–“\n",
    "Position 2: â–“â–‘â–“â–‘â–“â–‘\n",
    "Position 3: â–‘â–“â–‘â–“â–‘â–“\n",
    "...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Final Input:**\n",
    "\n",
    "```\n",
    "Word Embedding:       [0.2, 0.5, 0.8, 0.1, ...]\n",
    "+\n",
    "Positional Encoding:  [0.0, 1.0, 0.0, 1.0, ...]\n",
    "=\n",
    "Final Input:          [0.2, 1.5, 0.8, 1.1, ...]\n",
    "\n",
    "Now the model knows:\n",
    "- WHAT the word is (word embedding)\n",
    "- WHERE it is (positional encoding)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ¯ **PART 6: The Encoder - Understanding Input**\n",
    "\n",
    "## **Purpose:**\n",
    "\n",
    "Convert input text into **rich representations** that capture:\n",
    "- Word meanings\n",
    "- Context\n",
    "- Relationships\n",
    "- Grammar\n",
    "- Semantics\n",
    "\n",
    "---\n",
    "\n",
    "## **Single Encoder Layer:**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚         INPUT FROM BELOW             â”‚\n",
    "â”‚         (Word Embeddings)            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                 â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚      MULTI-HEAD SELF-ATTENTION        â”‚\n",
    "â”‚  (Words look at each other)           â”‚\n",
    "â”‚                                        â”‚\n",
    "â”‚  \"cat\" looks at: The, sat, mat        â”‚\n",
    "â”‚  \"sat\" looks at: cat, on, mat         â”‚\n",
    "â”‚  ...                                   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                 â†“\n",
    "         Add & Normalize\n",
    "         (Residual Connection)\n",
    "                 â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚      FEED FORWARD NETWORK             â”‚\n",
    "â”‚  (Process each word independently)    â”‚\n",
    "â”‚                                        â”‚\n",
    "â”‚  Word â†’ Dense â†’ ReLU â†’ Dense          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                 â†“\n",
    "         Add & Normalize\n",
    "         (Residual Connection)\n",
    "                 â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚      OUTPUT TO NEXT LAYER             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Components:**\n",
    "\n",
    "### **1. Multi-Head Self-Attention**\n",
    "\n",
    "```\n",
    "Input: \"The cat sat\"\n",
    "\n",
    "Each word attends to all words:\n",
    "- \"The\" â†’ understands it modifies \"cat\"\n",
    "- \"cat\" â†’ understands it's the subject of \"sat\"\n",
    "- \"sat\" â†’ understands \"cat\" is the doer\n",
    "\n",
    "Output: Context-aware representations\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Add & Normalize (Residual Connection)**\n",
    "\n",
    "**Problem:** Deep networks can have vanishing gradients\n",
    "\n",
    "**Solution:** Skip connection!\n",
    "\n",
    "```\n",
    "Input: x\n",
    "       â†“\n",
    "    [Layer]\n",
    "       â†“\n",
    "    Output: layer(x)\n",
    "       â†“\n",
    "    Add: x + layer(x)  â† Skip connection\n",
    "       â†“\n",
    "    Normalize\n",
    "```\n",
    "\n",
    "**Why?**\n",
    "- Easier training (gradients flow better)\n",
    "- Preserves original information\n",
    "- Allows very deep networks (100+ layers)\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Feed Forward Network**\n",
    "\n",
    "```\n",
    "For each word independently:\n",
    "\n",
    "Input: [512 dimensions]\n",
    "       â†“\n",
    "Dense Layer 1: [2048 dimensions]\n",
    "       â†“\n",
    "ReLU Activation\n",
    "       â†“\n",
    "Dense Layer 2: [512 dimensions]\n",
    "       â†“\n",
    "Output: [512 dimensions]\n",
    "\n",
    "Expands then compresses:\n",
    "512 â†’ 2048 â†’ 512\n",
    " â†“      â†“      â†“\n",
    "Simple Complex Simple\n",
    "```\n",
    "\n",
    "**Purpose:**\n",
    "- Add non-linearity\n",
    "- Learn complex patterns\n",
    "- Process each position separately\n",
    "\n",
    "---\n",
    "\n",
    "## **Complete Encoder Stack:**\n",
    "\n",
    "```\n",
    "Input: \"The cat sat on the mat\"\n",
    "       â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Encoder Layer 1   â”‚  Learn basic patterns\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Encoder Layer 2   â”‚  Learn medium patterns\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Encoder Layer 3   â”‚  Learn complex patterns\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â†“\n",
    "         ...\n",
    "         â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Encoder Layer N   â”‚  Learn very abstract patterns\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â†“\n",
    "    Rich Representation\n",
    "    (Full understanding of input)\n",
    "```\n",
    "\n",
    "**Typical N:** 6 layers (BERT), 12 layers (GPT-3 small), 96 layers (GPT-3 large)\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ¨ **PART 7: The Decoder - Generating Output**\n",
    "\n",
    "## **Purpose:**\n",
    "\n",
    "Generate output text **one word at a time**, using:\n",
    "- Previously generated words\n",
    "- Encoder's understanding of input\n",
    "\n",
    "---\n",
    "\n",
    "## **Single Decoder Layer:**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚    OUTPUT SO FAR (partial)           â”‚\n",
    "â”‚    \"Le chat\"                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                 â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   MASKED MULTI-HEAD SELF-ATTENTION    â”‚\n",
    "â”‚   (Look at previous words only)       â”‚\n",
    "â”‚                                        â”‚\n",
    "â”‚   \"chat\" can see: \"Le\"                â”‚\n",
    "â”‚   \"chat\" CANNOT see: future words     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                 â†“\n",
    "         Add & Normalize\n",
    "                 â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   ENCODER-DECODER ATTENTION           â”‚\n",
    "â”‚   (Look at input from encoder)        â”‚\n",
    "â”‚                                        â”‚\n",
    "â”‚   \"chat\" looks at: \"cat\", \"sat\", ...  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                 â†“\n",
    "         Add & Normalize\n",
    "                 â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚      FEED FORWARD NETWORK             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                 â†“\n",
    "         Add & Normalize\n",
    "                 â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚         NEXT WORD PREDICTION          â”‚\n",
    "â”‚         \"s'est\" (predicted)           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Difference from Encoder:**\n",
    "\n",
    "### **1. Masked Self-Attention**\n",
    "\n",
    "**Problem:** During generation, future words don't exist yet!\n",
    "\n",
    "**Solution:** Mask future positions\n",
    "\n",
    "```\n",
    "Generating: \"Le chat s'est assis\"\n",
    "\n",
    "When predicting \"s'est\":\n",
    "â”œâ”€ Can see: \"Le\", \"chat\" âœ“\n",
    "â””â”€ Cannot see: \"assis\" âœ— (future word)\n",
    "\n",
    "Attention Mask:\n",
    "         Le  chat s'est assis\n",
    "Le      [ 1,  0,   0,    0  ]  âœ“ See self only\n",
    "chat    [ 1,  1,   0,    0  ]  âœ“ See Le, chat\n",
    "s'est   [ 1,  1,   1,    0  ]  âœ“ See Le, chat, s'est\n",
    "assis   [ 1,  1,   1,    1  ]  âœ“ See all\n",
    "\n",
    "1 = can attend\n",
    "0 = masked (cannot attend)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Encoder-Decoder Attention**\n",
    "\n",
    "**Purpose:** Look at the input (from encoder)\n",
    "\n",
    "```\n",
    "Translating: \"The cat sat\" â†’ \"Le chat s'est assis\"\n",
    "\n",
    "When generating \"s'est\":\n",
    "â”œâ”€ Query (Q): Current word \"s'est\"\n",
    "â”œâ”€ Key (K): Input words [\"The\", \"cat\", \"sat\"]\n",
    "â””â”€ Value (V): Encoder representations\n",
    "\n",
    "Q(\"s'est\") asks K(input):\n",
    "\"What did 'sat' mean in English?\"\n",
    "â†’ Finds \"sat\" with high attention\n",
    "â†’ Uses that to generate \"s'est assis\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Generation Process:**\n",
    "\n",
    "```\n",
    "Translation: \"Cat sat\" â†’ \"Chat assis\"\n",
    "\n",
    "Step 1: Generate first word\n",
    "Input: [START]\n",
    "Encoder: \"Cat sat\"\n",
    "Output: \"Chat\" (predicted)\n",
    "\n",
    "Step 2: Generate second word  \n",
    "Input: [START, \"Chat\"]\n",
    "Encoder: \"Cat sat\"\n",
    "Output: \"assis\" (predicted)\n",
    "\n",
    "Step 3: Generate end token\n",
    "Input: [START, \"Chat\", \"assis\"]\n",
    "Encoder: \"Cat sat\"\n",
    "Output: [END] (predicted)\n",
    "\n",
    "Final: \"Chat assis\" âœ“\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ—ï¸ **PART 8: Complete Transformer Architecture**\n",
    "\n",
    "## **The Full Picture:**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    TRANSFORMER MODEL                        â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                             â”‚\n",
    "â”‚  INPUT                           OUTPUT (during training)   â”‚\n",
    "â”‚  \"The cat sat\"                   \"Le chat s'est assis\"     â”‚\n",
    "â”‚       â†“                                    â†“                â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚\n",
    "â”‚  â”‚  Embed  â”‚                         â”‚  Embed  â”‚          â”‚\n",
    "â”‚  â”‚    +    â”‚                         â”‚    +    â”‚          â”‚\n",
    "â”‚  â”‚  PosEnc â”‚                         â”‚  PosEnc â”‚          â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜          â”‚\n",
    "â”‚       â†“                                    â†“                â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚\n",
    "â”‚  â”‚   ENCODER    â”‚                   â”‚   DECODER    â”‚      â”‚\n",
    "â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚      â”‚\n",
    "â”‚  â”‚  â”‚ Layer 1â”‚  â”‚                   â”‚  â”‚ Layer 1â”‚  â”‚      â”‚\n",
    "â”‚  â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚                   â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚      â”‚\n",
    "â”‚  â”‚  â”‚ Layer 2â”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚  â”‚ Layer 2â”‚  â”‚      â”‚\n",
    "â”‚  â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚  (Cross Attn)     â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚      â”‚\n",
    "â”‚  â”‚  â”‚ Layer 3â”‚  â”‚                   â”‚  â”‚ Layer 3â”‚  â”‚      â”‚\n",
    "â”‚  â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚                   â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚      â”‚\n",
    "â”‚  â”‚  â”‚  ...   â”‚  â”‚                   â”‚  â”‚  ...   â”‚  â”‚      â”‚\n",
    "â”‚  â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚                   â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚      â”‚\n",
    "â”‚  â”‚  â”‚ Layer Nâ”‚  â”‚                   â”‚  â”‚ Layer Nâ”‚  â”‚      â”‚\n",
    "â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚      â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚\n",
    "â”‚                                             â†“              â”‚\n",
    "â”‚                                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚\n",
    "â”‚                                      â”‚   Linear    â”‚       â”‚\n",
    "â”‚                                      â”‚  + Softmax  â”‚       â”‚\n",
    "â”‚                                      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜       â”‚\n",
    "â”‚                                             â†“              â”‚\n",
    "â”‚                                   Probability Distribution â”‚\n",
    "â”‚                                   [0.01, 0.05, 0.89, ...]  â”‚\n",
    "â”‚                                             â†“              â”‚\n",
    "â”‚                                      Next Word: \"chat\"     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Information Flow:**\n",
    "\n",
    "```\n",
    "1. INPUT â†’ ENCODER\n",
    "   â”œâ”€ \"The cat sat\"\n",
    "   â”œâ”€ Add positional encoding\n",
    "   â”œâ”€ Process through N encoder layers\n",
    "   â””â”€ Output: Rich representation of input\n",
    "   \n",
    "2. ENCODER â†’ DECODER (Cross Attention)\n",
    "   â”œâ”€ Decoder can \"look at\" encoder output\n",
    "   â””â”€ Understands what needs to be translated\n",
    "   \n",
    "3. OUTPUT â†’ DECODER\n",
    "   â”œâ”€ Previously generated words\n",
    "   â”œâ”€ Add positional encoding\n",
    "   â””â”€ Process through N decoder layers\n",
    "   \n",
    "4. DECODER â†’ PREDICTION\n",
    "   â”œâ”€ Linear layer: [512] â†’ [vocab_size]\n",
    "   â”œâ”€ Softmax: Convert to probabilities\n",
    "   â””â”€ Select next word\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Training vs Inference:**\n",
    "\n",
    "### **Training (Teacher Forcing):**\n",
    "\n",
    "```\n",
    "Input: \"Cat sat\"\n",
    "Output (given): \"Chat assis\"\n",
    "\n",
    "Step 1: Predict word 1\n",
    "Decoder input: [START]\n",
    "Target: \"Chat\"\n",
    "Loss: CrossEntropy(prediction, \"Chat\")\n",
    "\n",
    "Step 2: Predict word 2  \n",
    "Decoder input: [START, \"Chat\"]  â† Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c10aaaf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## Gen AI Applications\n",
    "1. RAG Applications - Vector DB\n",
    "2. Mutimodal Applications - text, image, video, music\n",
    "3. AI Agents - muti AI agents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b3bd5a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fea86b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6e0edf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ef4bd0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
